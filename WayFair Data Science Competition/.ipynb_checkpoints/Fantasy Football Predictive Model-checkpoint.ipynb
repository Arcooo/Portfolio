{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href>https://www.lineups.com/nfl/roster/new-england-patriots</a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my imports\n",
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# NOTES while coding\n",
    "# imports I tried\n",
    "# from lxml.html.clean import clean_html\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# the location of a webdriver on my computer\n",
    "# this thing is probably what I need if I want to extract from complex websites where forms, and buttons, and parameters\n",
    "# need to be changed on the website.\n",
    "# driver = webdriver.Chrome('C:/NEW PROGRAMS/chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "# if I wanted to replace a certain letter in a string\n",
    "# and how to make things lowercase\n",
    "# for i, playerName in enumerate(players):\n",
    "# #     replace spaces with a dash and make name lowercase\n",
    "#     players[i] = players[i].replace(\" \", \"-\").lower()\n",
    "\n",
    "# An example of what a x --\n",
    "# /html/body/app-root/app-nfl/app-roster/div/div/div[2]/div[2]/div/div/table/tbody/tr[1] -- xpat code from a website\n",
    "# looks like\n",
    "\n",
    "# get the current date and time\n",
    "# current_time = datetime.datetime.now()\n",
    "\n",
    "# enumerated for loop\n",
    "# for i, player in enumerate(playerData):\n",
    "#     print (i, player)\n",
    "\n",
    "# a nooby way to get rid of spaces and enters\n",
    "# for each in playerInfo[0].text_content():\n",
    "#     if(each != ' ' and each != '\\n'):\n",
    "#         print(each)\n",
    "\n",
    "# URL \n",
    "# baseURL = \"https://www.lineups.com/nfl/player-stats/\"\n",
    "# tempUrl = baseURL + players[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pos', 'Name', 'Number', 'Rating', 'Ranking', 'Depth', 'Height', 'Weight', 'Age', 'Birthday', 'Exp.', 'Drafted', 'Draft Round', 'Draft Pick', 'College']\n"
     ]
    }
   ],
   "source": [
    "# first you have the URL of the site you want to webscrape from.\n",
    "url = \"https://www.lineups.com/nfl/roster/new-england-patriots\"\n",
    "# then you send a get request for the web html document and that comes back to you as a string\n",
    "page = requests.get(url).text\n",
    "# and then it turns that string into html formatted Python print and Tag-attached code. \n",
    "soup = BeautifulSoup(page, 'lxml')\n",
    "# print(soup) # see soup\n",
    "\n",
    "# I now have the document on my computer to be picked clean of what I need.\n",
    "# NFL 2018-2019 Data extraction to be used in Excel. Will also need pandas Dataframe for transition from code to excel.\n",
    "\n",
    "# Get headers\n",
    "headersInfo = soup.find('thead')\n",
    "headerCols = headersInfo.findAll('a')\n",
    "\n",
    "# so what I am doing here is naming a variable headersInfo to find the first @thead in the document\n",
    "# In this particular table, in thead, the first @a in the document numerically in the code after the html element @thead \n",
    "# is the info we need. The @a tagged elements need to be clicked in the original document so that you are able to sort by \n",
    "# relevance. \n",
    "\n",
    "# next we .findAll the a elements in the \"new\" document because thats the tag we need.\n",
    "# now that we have a -list of all the table data we need for our excel file of NFL data,\n",
    "# we need to GET THE TEXT FOR THE HEADERS AND SAVE IT IN THE TOP OF THE DATAFRAME I NEED TO USE\n",
    "\n",
    "headerColList = []\n",
    "for i,headerCol in enumerate(headerCols):\n",
    "#     print(headerCols[i].text.strip()) # to see the headerCols in good format\n",
    "    headerColList.append(headerCols[i].text.strip())\n",
    "    \n",
    "# print(headerColList) # The header for the dataframe for excel\n",
    "\n",
    "# I am getting the team data for the 2019-20 season. \n",
    "# I can see some upcoming problems. \n",
    "# I am going to have to figure out how to use chromeDriver or another way to switch the html buttons \n",
    "# Also, I would also like to put player stats data for the 2018-2019 season which happened last year. The stats are set\n",
    "# up to present the 2018-2019 season while the team is set up for the current year which hasn't started yet.\n",
    "# This means that the tables won't match up exactly, which is ok. Some of the players won't have any data in the beginning.\n",
    "# Well if there is a difference than that means they were around the year before, so players without data are new. \n",
    "\n",
    "# ok now I need to get my actual player data of the 2019-2020 roster without stats, just info = PlayerInformationTable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for i, each in enumerate(headerCols):\n",
    "#     print(i, each)\n",
    "# print(headerCols[0].text)\n",
    "# print(soup.prettify())\n",
    "\n",
    "# I now have the docu\n",
    "\n",
    "# my_table = soup.find('tbody')\n",
    "# rows = my_table.findAll('tr',{'class':'t-content'})\n",
    "# pos = rows[0].findAll('td')\n",
    "# name = rows[0].findAll('span',{'class':'player-name-col-lg'})\n",
    "# print(name[0].string)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for i, each in enumerate(pos):\n",
    "#     print(i, each)\n",
    "\n",
    "# print(len(pos))\n",
    "\n",
    "\n",
    "# print(type(my_table))\n",
    "\n",
    "# soup = BeautifulSoup(my_table, 'lxml')\n",
    "\n",
    "\n",
    "\n",
    "# rows = my_table.findAll('tr',{'class':'t-content'})\n",
    "\n",
    "\n",
    "# data = rows.find_all('td')\n",
    "# print(type(soup))\n",
    "# print(type(rows))\n",
    "\n",
    "\n",
    "# print(data[0])\n",
    "\n",
    "# pos = my_table.findAll('td',{'class','text-left px-3'})\n",
    "# # name = my_table.find('')\n",
    "# print(pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree = html.fromstring(page.content)\n",
    "\n",
    "# playerInfo = tree.xpath('//tbody') \n",
    "# # print(playerInfo[0].text_content()[0:100])\n",
    "\n",
    "# for each in playerInfo[0].text_content():\n",
    "#     if(each != ' ' and each != '\\n'):\n",
    "#         print(each)\n",
    "\n",
    "# print(type(playerInfo[0].text_content()))\n",
    "\n",
    "\n",
    "# print(playerInfo[0].xpath('//td')[0])\n",
    "\n",
    "# players = tree.xpath('//span[@class = \"player-name-col-lg\"]/text()')\n",
    "\n",
    "\n",
    "# for i, playerName in enumerate(players):\n",
    "# #     replace spaces with a dash and make name lowercase\n",
    "#     players[i] = players[i].replace(\" \", \"-\").lower()\n",
    "\n",
    "# print(players[0])\n",
    "\n",
    "# baseURL = \"https://www.lineups.com/nfl/player-stats/\"\n",
    "# tempUrl = baseURL + players[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(tempUrl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
