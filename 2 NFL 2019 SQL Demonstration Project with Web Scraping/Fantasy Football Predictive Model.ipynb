{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Web Scraping Notebook</h2>\n",
    "\n",
    "Scraped the NFL data from: \n",
    "<a href>https://www.lineups.com/nfl/roster/new-england-patriots</a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my imports\n",
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Requesting data from website only once</h3>\n",
    "\n",
    "You should request data and save the data on your local computer because pinging the website with to many requests will mark you as spam. You can change your User Agent if that happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageRequestData():\n",
    "    # first you have the URL of the site you want to webscrape from.\n",
    "    url = \"https://www.lineups.com/nfl/roster/new-england-patriots\"\n",
    "\n",
    "    # then you send a get request for the web html document and that comes back to you as a string\n",
    "    page = requests.get(url).text\n",
    "\n",
    "    # and then it turns that string into html formatted Python print and Tag-attached code. \n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "# url, page, soup = pageRequestData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>GitHub cannot sent page requests</h3>\n",
    "\n",
    "I need to save soup as an html file on my local machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"nfl_ne_roster.html\"\n",
    "\n",
    "# with open(filename, \"w\") as file:\n",
    "#     file.write(str(soup))\n",
    "\n",
    "soupString = open(filename, \"r\").read()\n",
    "\n",
    "# turn back into BeautifulSoup\n",
    "soup = BeautifulSoup(soupString, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get headers for excel file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Spreadsheet Headers----\n",
      "['Team', 'Pos', 'Name', 'Number', 'Rating', 'Ranking', 'Depth', 'Height', 'Weight', 'Age', 'Birthday', 'Exp.', 'Drafted', 'Draft Round', 'Draft Pick', 'College']\n"
     ]
    }
   ],
   "source": [
    "# Get headers\n",
    "headersInfo = soup.find('thead')\n",
    "headerCols = headersInfo.findAll('a')\n",
    "\n",
    "# so what I am doing here is naming a variable headersInfo to find the first @thead in the document\n",
    "# In this particular table, in thead, the first @a in the document numerically in the code after the html element @thead \n",
    "# is the info we need. The @a tagged elements need to be clicked in the original document so that you are able to sort by \n",
    "# relevance. \n",
    "\n",
    "# next we .findAll the a elements in the \"new\" document because thats the tag we need.\n",
    "# now that we have a -list of all the table data we need for our excel file of NFL data,\n",
    "# we need to GET THE TEXT FOR THE HEADERS AND SAVE IT IN THE TOP OF THE DATAFRAME I NEED TO USE\n",
    "\n",
    "# make a list for the headers\n",
    "headerColList = []\n",
    "\n",
    "# append as the first element Team which is not in the table\n",
    "headerColList.append(\"Team\")\n",
    "\n",
    "for i,headerCol in enumerate(headerCols):\n",
    "#     print(headerCols[i].text.strip()) # to see the headerCols in good format\n",
    "    headerColList.append(headerCols[i].text.strip())\n",
    "\n",
    "print(\"----Spreadsheet Headers----\")\n",
    "print(headerColList) # The header for the dataframe for excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get data of each player in the roster data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NE', 'QB', 'Tom Brady', 12, 95, '#1 QB', 1, '6\\'4\"', 225, 42, '8/3/77', 20, 2000, 6, 199, 'Michigan']\n"
     ]
    }
   ],
   "source": [
    "team = \"NE\"\n",
    "\n",
    "# I am getting the team data for the 2019-20 season. \n",
    "# I can see some upcoming problems. \n",
    "# I am going to have to figure out how to use chromeDriver or another way to switch the html buttons \n",
    "# Also, I would also like to put player stats data for the 2018-2019 season which happened last year. The stats are set\n",
    "# up to present the 2018-2019 season while the team is set up for the current year which hasn't started yet.\n",
    "# This means that the tables won't match up exactly, which is ok. Some of the players won't have any data in the beginning.\n",
    "# Well if there is a difference than that means they were around the year before, so players without data are new. \n",
    "\n",
    "# ok now I need to get my actual player data of the 2019-2020 roster without stats, just info = PlayerInformationTable\n",
    "# The code below finds the first tbody element in the soup document which is what we need for the data\n",
    "PlayerRosterInfoData = soup.find('tbody')\n",
    "\n",
    "# Make a list of all the 'tr' rows which are each individual players info\n",
    "t_content_rows = PlayerRosterInfoData.findAll('tr',{'class':'t-content'})\n",
    "# print(len(t_content_rows)) # number of players in table\n",
    "\n",
    "playerData = []\n",
    "\n",
    "# for each player\n",
    "for i,each_player in enumerate(t_content_rows):\n",
    "#     get all the column info for each found in the @td tag\n",
    "    player_row = each_player.findAll('td')\n",
    "\n",
    "    tempData = []\n",
    "    tempData.append(team)\n",
    "\n",
    "#     each td contains info for the dataframe\n",
    "    for i,col in enumerate(player_row):\n",
    "        \n",
    "#       the players name at indices 1 needs to be formated correctly\n",
    "        if (i!=1):\n",
    "#             print(col.text.strip())\n",
    "\n",
    "#             turn string digits into digits\n",
    "            if(col.text.strip().isdigit()):\n",
    "                tempData.append(int(col.text.strip()))\n",
    "            else:\n",
    "                tempData.append(col.text.strip())\n",
    "        else:\n",
    "            playerName_re = re.compile('\\S+\\s\\S+') \n",
    "            tempData.append(playerName_re.match(col.text.strip()).group())\n",
    "#             print(playerName_re.match(col.text.strip()).group())\n",
    "\n",
    "    playerData.append(tempData)\n",
    "print(playerData[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Put data into dataframe and export it as an excel sheet</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(playerData, columns = headerColList)\n",
    "df.to_excel('output.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Coding Notes </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES while coding\n",
    "# imports I tried\n",
    "# from lxml.html.clean import clean_html\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# the location of a webdriver on my computer\n",
    "# this thing is probably what I need if I want to extract from complex websites where forms, and buttons, and parameters\n",
    "# need to be changed on the website.\n",
    "# driver = webdriver.Chrome('C:/NEW PROGRAMS/chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "# if I wanted to replace a certain letter in a string\n",
    "# and how to make things lowercase\n",
    "# for i, playerName in enumerate(players):\n",
    "# #     replace spaces with a dash and make name lowercase\n",
    "#     players[i] = players[i].replace(\" \", \"-\").lower()\n",
    "\n",
    "# An example of what a x --\n",
    "# /html/body/app-root/app-nfl/app-roster/div/div/div[2]/div[2]/div/div/table/tbody/tr[1] -- xpat code from a website\n",
    "# looks like\n",
    "\n",
    "# get the current date and time\n",
    "# current_time = datetime.datetime.now()\n",
    "\n",
    "# enumerated for loop\n",
    "# for i, player in enumerate(playerData):\n",
    "#     print (i, player)\n",
    "\n",
    "# a nooby way to get rid of spaces and enters\n",
    "# for each in playerInfo[0].text_content():\n",
    "#     if(each != ' ' and each != '\\n'):\n",
    "#         print(each)\n",
    "\n",
    "# URL \n",
    "# baseURL = \"https://www.lineups.com/nfl/player-stats/\"\n",
    "# tempUrl = baseURL + players[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree = html.fromstring(page.content)\n",
    "\n",
    "# playerInfo = tree.xpath('//tbody') \n",
    "# # print(playerInfo[0].text_content()[0:100])\n",
    "\n",
    "# for each in playerInfo[0].text_content():\n",
    "#     if(each != ' ' and each != '\\n'):\n",
    "#         print(each)\n",
    "\n",
    "# print(type(playerInfo[0].text_content()))\n",
    "\n",
    "\n",
    "# print(playerInfo[0].xpath('//td')[0])\n",
    "\n",
    "# players = tree.xpath('//span[@class = \"player-name-col-lg\"]/text()')\n",
    "\n",
    "\n",
    "# for i, playerName in enumerate(players):\n",
    "# #     replace spaces with a dash and make name lowercase\n",
    "#     players[i] = players[i].replace(\" \", \"-\").lower()\n",
    "\n",
    "# print(players[0])\n",
    "\n",
    "# baseURL = \"https://www.lineups.com/nfl/player-stats/\"\n",
    "# tempUrl = baseURL + players[0]\n",
    "\n",
    "# print(tempUrl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
