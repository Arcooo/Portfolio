{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my imports\n",
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Fragements (Skip down)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = requests.Session()\n",
    "# page = requests.get('https://www.lineups.com/nfl/player-stats/matt-ryan')\n",
    "# # print(page)\n",
    "# soup = BeautifulSoup(page, 'lxml')\n",
    "# print(soup)\n",
    "\n",
    "# from lxml.html.clean import clean_html\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# # the location of a webdriver on my computer\n",
    "# # this thing is probably what I need if I want to extract from complex websites where forms, and buttons, and parameters\n",
    "# # need to be changed on the website.\n",
    "# driver = webdriver.Chrome('C:/NEW PROGRAMS/chromedriver_win32/chromedriver.exe')\n",
    "\n",
    "# driver.get(\"https://www.lineups.com/nfl/roster/new-england-patriots\")\n",
    "# # driver.get(\"https://www.lineups.com/nfl/player-stats/Matt-Ryan\")\n",
    "\n",
    "#     time.sleep()\n",
    "\n",
    "#     filename = \"nfl_ne_roster.html\"\n",
    "#     soupString = open(filename, \"r\").read()\n",
    "#     soup = BeautifulSoup(soupString, \"html.parser\")\n",
    "\n",
    "# pos_df = pd.DataFrame(position_data)\n",
    "# pos_df.fillna(\"Rookie\", inplace=True)\n",
    "# pos_df.columns = position_headers\n",
    "\n",
    "# # filename = position + '-stats.xlsx'.xlsx\n",
    "# print(filename)\n",
    "# pos_df.to_excel(filename)\n",
    "\n",
    "# position_data\n",
    "# for each in position_data:\n",
    "#     if len(each)>9:\n",
    "#         print (each)\n",
    "# print(df.loc[df['Name'] == 'John Franklin'])\n",
    "\n",
    "# # with open(filename, \"w\") as file:\n",
    "# #     file.write(str(soup))\n",
    "\n",
    "# soupString = open(filename, \"r\").read()\n",
    "\n",
    "# # turn back into BeautifulSoup\n",
    "# soup = BeautifulSoup(soupString, \"html.parser\")\n",
    "# pos_unique = list(df['Pos'].unique())\n",
    "# print(type(pos_unique[14]))\n",
    "# pos_df = df['Pos'] == 'OL'\n",
    "# pos_df = df[pos_df]\n",
    "# print(len(pos_df))\n",
    "# print(pos_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the text file nflTeams.txt that is a list of all 32 NFL teams\n",
    "\n",
    "Then, format the textfile into a list of properly formed URL endings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arizona-cardinals', 'atlanta-falcons', 'baltimore-ravens', 'buffalo-bills', 'carolina-panthers']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"nflTeams.txt\", \"r\")\n",
    "nflTeams = file.read().splitlines()\n",
    "\n",
    "for i, team in enumerate(nflTeams):\n",
    "    nflTeams[i] = nflTeams[i].replace(\" \", \"-\").lower()\n",
    "\n",
    "print(nflTeams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get roster data from all NFL teams: https://www.lineups.com/nfl/roster/\n",
    "\n",
    "Then export it to an excel file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "base_url = \"https://www.lineups.com/nfl/roster/\"\n",
    "teamData = []\n",
    "\n",
    "for team in nflTeams:\n",
    "    pass\n",
    "    print(team)\n",
    "    \n",
    "    url = base_url + team\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    \n",
    "    playerRosterInfoData = soup.find('tbody')\n",
    "    t_content_rows = playerRosterInfoData.findAll('tr',{'class':'t-content'})\n",
    "    \n",
    "    playerData = []\n",
    "\n",
    "    # for each player\n",
    "    for i,each_player in enumerate(t_content_rows):\n",
    "    #     get all the column info for each found in the @td tag\n",
    "        player_row = each_player.findAll('td')\n",
    "\n",
    "        tempData = []\n",
    "        tempData.append(team)\n",
    "\n",
    "    #     each td contains info for the dataframe\n",
    "        for i,col in enumerate(player_row):\n",
    "\n",
    "    #       the players name at indices 1 needs to be formated correctly\n",
    "            if (i!=1):\n",
    "    #             print(col.text.strip())\n",
    "\n",
    "    #             turn string digits into digits\n",
    "                if(col.text.strip().isdigit()):\n",
    "                    tempData.append(int(col.text.strip()))\n",
    "                else:\n",
    "                    tempData.append(col.text.strip())\n",
    "            else:\n",
    "                playerName_re = re.compile('\\S+\\s\\S+') \n",
    "                tempData.append(playerName_re.match(col.text.strip()).group())\n",
    "    #             print(playerName_re.match(col.text.strip()).group())\n",
    "\n",
    "        playerData.append(tempData)\n",
    "    teamData = teamData + playerData\n",
    "\n",
    "    # wait request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total NFL players"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(teamData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the headers for the Roster Data\n",
    "\n",
    "And print"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# headers\n",
    "headersInfo = soup.find('thead')\n",
    "headerCols = headersInfo.findAll('a')\n",
    "headerColList = []\n",
    "headerColList.append(\"Team\")\n",
    "for i,headerCol in enumerate(headerCols):\n",
    "    headerColList.append(headerCols[i].text.strip())\n",
    "print(headerColList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe of data and export to excel file named:\n",
    "\n",
    "    'nfl_rosters.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(teamData, columns = headerColList)\n",
    "# df.to_excel('nfl_rosters.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting the Positional Stats of Each Player</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the made excel sheet from previous code into notebook\n",
    "\n",
    "(So you don't have to rescrape every time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"nfl_rosters.xlsx\"\n",
    "df = pd.read_excel(filename)\n",
    "# print(df.tail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all the positions that I am making a separate table for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'CB', 'DB', 'DE', 'DL', 'DT', 'FB', 'FS', 'G', 'ILB', 'K', 'LB', 'LS', 'NT', 'OL', 'OLB', 'OT', 'P', 'QB', 'RB', 'S', 'SS', 'TE', 'WR', nan]\n"
     ]
    }
   ],
   "source": [
    "pos_unique = list(df['Pos'].unique())\n",
    "print(pos_unique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the player stats and export it to an excel file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pos_unique = list(df['Pos'].unique())\n",
    "\n",
    "if 'nan' in pos_unique:\n",
    "    pos_unique.remove('nan')\n",
    "\n",
    "base_url = \"https://www.lineups.com/nfl/player-stats/\"\n",
    "\n",
    "\n",
    "for i, position in enumerate(pos_unique[15:16]):\n",
    "    print(position+ '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    pos_df = df['Pos'] == position\n",
    "    pos_df = df[pos_df]\n",
    "    \n",
    "    # get headers for the position\n",
    "    position_headers = []\n",
    "    count = 0\n",
    "    while(True):\n",
    "        player = pos_df['Name'].iloc[count]\n",
    "        url = base_url + player.replace(\" \" ,\"-\").replace(\"'\" , '').lower()\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        pos_info = soup.find(\"thead\")\n",
    "        \n",
    "        tempData = []\n",
    "        tempData.append('Name')\n",
    "        if (pos_info != None):   \n",
    "            pos_th = pos_info.findAll('th')\n",
    "\n",
    "            for th in pos_th:\n",
    "                tempData.append(th.text.strip())\n",
    "            position_headers = tempData\n",
    "            break        \n",
    "        count += 1\n",
    "        \n",
    "        if count == len(pos_df):\n",
    "            position_headers = tempData\n",
    "            break\n",
    "\n",
    "    # get stats\n",
    "    position_data = []\n",
    "    \n",
    "    for player in pos_df['Name']:\n",
    "        print(player)\n",
    "        tempData = []\n",
    "        tempData.append(player)\n",
    "        \n",
    "        url = base_url + player.replace(\" \" ,\"-\").replace(\"'\" , '').lower()\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        pos_info = soup.find(\"tbody\")\n",
    "        \n",
    "        if (pos_info != None):   \n",
    "            pos_td = pos_info.findAll('td')\n",
    "            for td in pos_td:\n",
    "                if(td.text.strip().isdigit()):\n",
    "                    tempData.append(int(td.text.strip()))\n",
    "                else:\n",
    "                    tempData.append(td.text.strip())\n",
    "        position_data.append(tempData)\n",
    "    \n",
    "    pos_df = pd.DataFrame(position_data)\n",
    "    pos_df.fillna(\"Rookie\", inplace=True)\n",
    "    pos_df.columns = position_headers\n",
    "    \n",
    "    filename = position + '_stats.xlsx'\n",
    "    pos_df.to_excel(filename)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
